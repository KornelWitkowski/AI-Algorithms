{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "5_deep_deterministic_policy_gradient.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Deterministic Policy Gradient (DDPG)"
      ],
      "metadata": {
        "id": "3xt6fIDownZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb\n",
        "\n",
        "!pip install gym==0.21 \\\n",
        "    pytorch-lightning==1.6.0 \\\n",
        "    pyvirtualdisplay\n",
        "\n",
        "!pip install git+https://github.com/google/brax.git@main"
      ],
      "metadata": {
        "id": "6RqrzokqoanP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOSJl-X7zvs4"
      },
      "source": [
        "#### Setup virtual display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-Z6takfzqGk"
      },
      "outputs": [],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "Display(visible=False, size=(1400, 900)).start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cz8DLleGz_TF"
      },
      "source": [
        "#### Import the necessary code libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "cP5t6U7-nYoc"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import gym\n",
        "import torch\n",
        "import random\n",
        "import functools\n",
        "\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from collections import deque, namedtuple\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data.dataset import IterableDataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "from pytorch_lightning import LightningModule, Trainer\n",
        "\n",
        "import brax\n",
        "from brax import envs\n",
        "from brax.envs import to_torch\n",
        "from brax.io import html\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "num_gpus = torch.cuda.device_count()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Z_IrPlU1wwPx"
      },
      "outputs": [],
      "source": [
        "def display_video(episode=0):\n",
        "  video_file = open(f'/content/videos/rl-video-episode-{episode}.mp4', \"r+b\").read()\n",
        "  video_url = f\"data:video/mp4;base64,{b64encode(video_file).decode()}\"\n",
        "  return HTML(f\"<video width=600 controls><source src='{video_url}'></video>\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entry_point = functools.partial(envs.create_gym_env, env_name='ant')\n",
        "gym.register('brax-ant-v0', entry_point=entry_point)"
      ],
      "metadata": {
        "id": "Vl5wpoAj1OK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_environment(env_name, num_envs=256, episode_length=1000):\n",
        "  env = gym.make(env_name, batch_size=num_envs, episode_length=episode_length)\n",
        "  env = to_torch.JaxToTorchWrapper(env, device=device)\n",
        "  return env"
      ],
      "metadata": {
        "id": "evrLpUqXKres"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def test_env(env_name, policy=None):\n",
        "  env = gym.make(env_name, episode_length=1000)\n",
        "  env = to_torch.JaxToTorchWrapper(env, device=device)\n",
        "  qp_array = []\n",
        "  state = env.reset()\n",
        "  for i in range(1000):\n",
        "    if policy:\n",
        "      action = algo.policy.net(state.unsqueeze(0)).squeeze()\n",
        "    else:\n",
        "      action = env.action_space.sample()\n",
        "    state, _, _, _ = env.step(action)\n",
        "    qp_array.append(env.unwrapped._state.qp)\n",
        "  return HTML(html.render(env.unwrapped._env.sys, qp_array))"
      ],
      "metadata": {
        "id": "QwYlpTOY1Ajo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create the gradient policy"
      ],
      "metadata": {
        "id": "-SmWkjyfs7kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientPolicy(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, out_dims, min, max):\n",
        "    super().__init__()\n",
        "    self.min = torch.from_numpy(min).to(device)\n",
        "    self.max = torch.from_numpy(max).to(device)\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, out_dims),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "\n",
        "  def mu(self, x):\n",
        "    if isinstance(x, np.ndarray):\n",
        "      x = torch.from_numpy(x).to(device)\n",
        "    return self.net(x.float()) * self.max\n",
        "\n",
        "  def forward(self, x, epsilon=0.0):\n",
        "    mu = self.mu(x)\n",
        "    mu = mu + torch.normal(0, epsilon, mu.size(), device=mu.device)\n",
        "    action = torch.max(torch.min(mu, self.max), self.min)\n",
        "    action = action.cpu().numpy()\n",
        "    return action\n"
      ],
      "metadata": {
        "id": "mNDbTLeZuP1m"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create the Deep Q-Network"
      ],
      "metadata": {
        "id": "0dv2XzwmtB3r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "\n",
        "  def __init__(self, hidden_size, obs_size, out_dims):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(obs_size + out_dims, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, hidden_size),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(hidden_size, 1)\n",
        "    )\n",
        "\n",
        "  def forward(self, state, action):\n",
        "    if isinstance(state, np.ndarray):\n",
        "      state = torch.from_numpy(state).to(device)\n",
        "    if isinstance(action, np.ndarray):\n",
        "      action = torch.from_numpy(action).to(device)\n",
        "    in_vector = torch.hstack((state, action))\n",
        "    return self.net(in_vector.float())\n"
      ],
      "metadata": {
        "id": "k9Z8OviE8V1A"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBuffer:\n",
        "\n",
        "  def __init__(self, capacity):\n",
        "    self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.buffer)\n",
        "\n",
        "  def append(self, experience):\n",
        "    self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "    return random.sample(self.buffer, batch_size)"
      ],
      "metadata": {
        "id": "8XxdqufquQK6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RLDataset(IterableDataset):\n",
        "\n",
        "  def __init__(self, buffer, sample_size=200):\n",
        "    self.buffer = buffer\n",
        "    self.sample_size = sample_size\n",
        "\n",
        "  def __iter__(self):\n",
        "    for experience in self.buffer.sample(self.sample_size):\n",
        "      yield experience"
      ],
      "metadata": {
        "id": "roQfFswgKAZC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def polyak_average(net, target_net, tau=0.01):\n",
        "  for qp, tp in zip(net.parameters(), target_net.parameters()):\n",
        "    tp.data.copy_(tau * qp.data + (1 - tau) * tp.data)"
      ],
      "metadata": {
        "id": "O7URcCS7uQNc"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DDPG(LightningModule):\n",
        "  \n",
        "  def __init__(self, env_name, capacity=500, batch_size=8192, actor_lr=1e-3,\n",
        "               critic_lr=1e-3, hidden_size=256, gamma=0.99, loss_fn=F.smooth_l1_loss,\n",
        "               optim=AdamW, eps_start=1.0, eps_end=0.2, eps_last_episode=500, samples_per_epoch=10,\n",
        "               tau=0.005):\n",
        "    super().__init__()\n",
        "\n",
        "    self.env = create_environment(env_name, num_envs=batch_size)\n",
        "    self.obs = self.env.reset()\n",
        "    self.videos = []\n",
        "\n",
        "    obs_size = self.env.observation_space.shape[1]\n",
        "    action_dims = self.env.action_space.shape[1]\n",
        "    max_action = self.env.action_space.high\n",
        "    min_action = self.env.action_space.low\n",
        "\n",
        "    self.q_net = DQN(hidden_size, obs_size, action_dims)\n",
        "    self.policy = GradientPolicy(hidden_size, obs_size, action_dims, min_action, max_action)\n",
        "\n",
        "    self.target_policy = copy.deepcopy(self.policy)\n",
        "    self.target_q_net = copy.deepcopy(self.q_net)\n",
        "\n",
        "    self.buffer = ReplayBuffer(capacity=capacity)\n",
        "\n",
        "    self.save_hyperparameters()\n",
        "\n",
        "    while len(self.buffer) < self.hparams.samples_per_epoch:\n",
        "      print(f\"{len(self.buffer)} samples in experience buffer. Filling...\")\n",
        "      self.play(epsilon=self.hparams.eps_start)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def play(self, policy=None, epsilon=0.0):\n",
        "    if policy:\n",
        "      action = policy(self.obs, epsilon=epsilon)\n",
        "    else:\n",
        "      action = self.env.action_space.sample()\n",
        "    next_obs, reward, done, info = self.env.step(action)\n",
        "    exp = (self.obs, action, reward, done, next_obs)\n",
        "    self.buffer.append(exp)\n",
        "    self.obs = next_obs\n",
        "    return reward.mean()\n",
        "\n",
        "  def forward(self, x):\n",
        "    output = self.policy.mu(x)\n",
        "    return output\n",
        "\n",
        "  def configure_optimizers(self):\n",
        "    q_net_optimizer = self.hparams.optim(self.q_net.parameters(), lr=self.hparams.critic_lr)\n",
        "    policy_optimizer = self.hparams.optim(self.policy.parameters(), lr=self.hparams.actor_lr)\n",
        "    return [q_net_optimizer, policy_optimizer]\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    dataset = RLDataset(self.buffer, self.hparams.samples_per_epoch)\n",
        "    dataloader = DataLoader(dataset=dataset,\n",
        "                            batch_size=1)\n",
        "    return dataloader\n",
        "    \n",
        "  def training_step(self, batch, batch_idx, optimizer_idx):\n",
        "    epsilon = max(self.hparams.eps_end,\n",
        "                  self.hparams.eps_start - self.current_epoch / self.hparams.eps_last_episode)\n",
        "    \n",
        "    mean_reward = self.play(policy=self.policy, epsilon=epsilon)\n",
        "    self.log(\"episode/mean_reward.\", mean_reward)\n",
        "\n",
        "    polyak_average(self.q_net, self.target_q_net, tau=self.hparams.tau)\n",
        "    polyak_average(self.policy, self.target_policy, tau=self.hparams.tau)\n",
        "\n",
        "    states, actions, rewards, dones, next_states = map(torch.squeeze, batch)\n",
        "    rewards = rewards.unsqueeze(1)\n",
        "    dones = dones.unsqueeze(1).bool()\n",
        "\n",
        "    if optimizer_idx == 0:\n",
        "      action_values = self.q_net(states, actions)\n",
        "      next_actions = self.target_policy.mu(next_states)\n",
        "      next_action_values = self.target_q_net(next_states, next_actions)\n",
        "      next_action_values[dones] = 0.0\n",
        "\n",
        "      expected_action_values = rewards + self.hparams.gamma * next_action_values\n",
        "      q_loss = self.hparams.loss_fn(action_values, expected_action_values)\n",
        "      self.log(\"episode/Q-Loss\", q_loss)\n",
        "      return q_loss\n",
        "\n",
        "    elif optimizer_idx == 1:\n",
        "      mu = self.policy.mu(states)\n",
        "      policy_loss = - self.q_net(states, mu).mean()\n",
        "      self.log(\"episode/Policy Loss\", policy_loss)\n",
        "      return policy_loss    \n",
        "\n",
        "    def train_epoch_end(self, outputs):\n",
        "      if self.current_epoch % 100 == 0:\n",
        "        video = test_env(self.env.spec.id, policy=self.policy)\n",
        "        self.videos.append(video)\n",
        "\n"
      ],
      "metadata": {
        "id": "KtxZePn-uQQA"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r /content/lightning_logs/\n",
        "!rm -r /content/videos/\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/lightning_logs/"
      ],
      "metadata": {
        "id": "6F57mBk7gGBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "algo = DDPG(\"brax-ant-v0\")\n",
        "\n",
        "trainer = Trainer(\n",
        "  gpus=num_gpus,\n",
        "  max_epochs=5_000,\n",
        "  log_every_n_steps=10\n",
        ")\n",
        "\n",
        "trainer.fit(algo)"
      ],
      "metadata": {
        "id": "AH-QdcqQgv19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rhkJbR21g-GM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}